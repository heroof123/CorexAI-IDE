// AI Provider Management Service
import { invoke } from "@tauri-apps/api/core";

export interface AIProvider {
  id: string;
  name: string;
  type: 'openai' | 'anthropic' | 'local' | 'custom';
  baseUrl: string;
  host?: string;
  port?: number;
  apiKey?: string;
  models: AIModel[];
  isActive: boolean;
  icon: string;
  description: string;
}

export interface AIModel {
  id: string;
  name: string;
  displayName: string;
  description: string;
  specialty: string;
  roles?: ('coder' | 'tester' | 'planner' | 'chat' | 'reviewer' | 'analyzer')[]; // ğŸ†• Ã‡oklu roller
  maxTokens?: number;
  temperature?: number;
  isActive: boolean;
}

import { storage } from "./storage";

// AI Provider'larÄ± yÃ¼kle
export async function loadAIProviders(): Promise<AIProvider[]> {
  const saved = await storage.getSettings<AIProvider[]>('corex-ai-providers');
  if (saved) {
    return saved;
  }

  // Default providers
  const defaultProviders: AIProvider[] = [
    {
      id: "lm-studio",
      name: "LM Studio",
      type: "local",
      baseUrl: "http://127.0.0.1:1234/v1",
      host: "127.0.0.1",
      port: 1234,
      models: [
        {
          id: "main",
          name: "qwen2.5-coder-7b-instruct",
          displayName: "Qwen2.5 Coder 7B",
          description: "Ana model - Planlama ve kodlama",
          specialty: "Coder", // Rollere gÃ¶re gÃ¼ncellendi
          roles: ["coder"], // ğŸ†• Ã‡oklu roller
          maxTokens: 4096,
          temperature: 0.5,
          isActive: true
        },
        {
          id: "chat",
          name: "qwen2.5-3b-instruct",
          displayName: "Qwen2.5 3B",
          description: "HÄ±zlÄ± sohbet ve basit gÃ¶revler",
          specialty: "Chat", // Rollere gÃ¶re gÃ¼ncellendi
          roles: ["chat"], // ğŸ†• Ã‡oklu roller
          maxTokens: 8192, // ğŸ”¥ 2048'den 8192'ye Ã§Ä±karÄ±ldÄ± - kod yazarken yeterli olsun
          temperature: 0.7,
          isActive: true
        }
      ],
      isActive: true,
      icon: "ğŸ–¥ï¸",
      description: "Yerel LM Studio sunucusu"
    }
  ];

  await storage.setSettings('corex-ai-providers', defaultProviders);
  return defaultProviders;
}

// AI Provider'larÄ± kaydet
export async function saveAIProviders(providers: AIProvider[]): Promise<void> {
  await storage.setSettings('corex-ai-providers', providers);
}

// Aktif modeli bul
export async function findActiveModel(modelId: string): Promise<{ provider: AIProvider; model: AIModel } | null> {
  const providers = await loadAIProviders();

  // 1. Ã–nce ID ile tam eÅŸleÅŸen ve aktif olan modeli ara
  for (const provider of providers) {
    // Provider pasif olsa bile, eÄŸer model ID tam eÅŸleÅŸiyorsa ve model aktifse kabul et (Fallback)
    // Bu, GGUF provider'Ä±n bazen pasif kalmasÄ±na raÄŸmen modellerinin aktif olmasÄ± durumunu kurtarÄ±r.
    const model = provider.models.find(m => m.id === modelId && m.isActive);
    if (model) {
      // EÄŸer provider pasifse ama model aktifse, provider'Ä± geÃ§ici olarak aktif kabul et
      return { provider: { ...provider, isActive: true }, model };
    }
  }

  // 2. EÄŸer ID ile bulunamadÄ±ysa (veya ID 'default' ise), ilk aktif modelden devam et
  if (!modelId || modelId === 'default' || modelId === 'main' || modelId === 'chat') {
    for (const provider of providers) {
      if (!provider.isActive) continue;
      const model = provider.models.find(m => m.isActive);
      if (model) return { provider, model };
    }
  }

  return null;
}

// Dinamik AI Ã§aÄŸrÄ±sÄ± - provider ayarlarÄ±nÄ± kullanarak
// ğŸ†• Mesajdan resimleri parse et
function parseImagesFromMessage(message: string): { cleanMessage: string; images: string[] } {
  const imageRegex = /\[IMAGES:(\d+)\]\n((?:\[IMAGE_\d+\]:data:image\/[^;]+;base64,[^\n]+\n)+)/;
  const match = message.match(imageRegex);

  if (!match) {
    return { cleanMessage: message, images: [] };
  }

  const imageCount = parseInt(match[1]);
  const imagesBlock = match[2];
  const images: string[] = [];

  // Her bir resmi parse et
  const imageLines = imagesBlock.split('\n').filter(line => line.startsWith('[IMAGE_'));
  for (const line of imageLines) {
    const imageMatch = line.match(/\[IMAGE_\d+\]:(data:image\/[^;]+;base64,.+)/);
    if (imageMatch) {
      images.push(imageMatch[1]);
    }
  }

  // Mesajdan resim bloÄŸunu Ã§Ä±kar
  const cleanMessage = message.replace(imageRegex, '').trim();

  console.log('ğŸ“· Parse edildi:', { imageCount, foundImages: images.length, cleanMessageLength: cleanMessage.length });

  return { cleanMessage, images };
}

/**
 * ğŸ§¹ GGUF yanÄ±tlarÄ±nÄ± sanitize et (HalisÃ¼nasyonlarÄ± ve stop token kaÃ§aklarÄ±nÄ± engelle)
 */
function sanitizeGgufResponse(text: string): string {
  if (!text) return "";

  // YaygÄ±n stop belirteÃ§leri ve halisÃ¼nasyon kalÄ±plarÄ±
  const stopSequences = [
    "User:",
    "### User:",
    "Assistant:",
    "### Assistant:",
    "<|im_start|>",
    "<|im_end|>",
    "### Instruction:",
    "### Response:",
    "Q:",
    "A:",
    "\n\n\n", // AÅŸÄ±rÄ± boÅŸluk varsa kes (bazen loop'a girer)
  ];

  let cleaned = text;

  for (const seq of stopSequences) {
    const index = cleaned.indexOf(seq);
    if (index !== -1) {
      cleaned = cleaned.substring(0, index);
    }
  }

  return cleaned.trim();
}

/**
 * ğŸ” Metin iÃ§inden JSON bloÄŸunu ayÄ±kla ve parse et (FIX-34)
 */
export function extractJsonFromText<T>(text: string): T | null {
  if (!text) return null;

  try {
    // 1. Direkt parse etmeyi dene
    return JSON.parse(text);
  } catch {
    try {
      // 2. Markdown kod bloklarÄ±nÄ± ara (```json ... ```)
      const jsonBlockRegex = /```(?:json)?\s*([\s\S]*?)```/;
      const match = text.match(jsonBlockRegex);
      if (match && match[1]) {
        return JSON.parse(match[1].trim());
      }

      // 3. Ä°lk { ve son } arasÄ±nÄ± dene
      const firstBrace = text.indexOf('{');
      const lastBrace = text.lastIndexOf('}');
      if (firstBrace !== -1 && lastBrace !== -1 && lastBrace > firstBrace) {
        const potentialJson = text.substring(firstBrace, lastBrace + 1);
        // Temizlik: Kontrol karakterlerini ve geÃ§ersiz kaÃ§Ä±ÅŸlarÄ± temizle
        const cleanedJson = potentialJson
          .replace(/[\u0000-\u001F\u007F-\u009F]/g, "")
          .trim();
        return JSON.parse(cleanedJson);
      }
    } catch (e) {
      console.warn("âš ï¸ JSON extraction failed:", e);
    }
  }
  return null;
}

const getAgenticInstruction = (isTurkish: boolean): string => {
  return isTurkish
    ? 'Sen CorexAI asistanÄ±sÄ±n. EÄŸer sana sadece selam veriliyorsa veya kodla ilgisiz bir sohbet ediliyorsa, doÄŸal bir dille sadece sohbet et, asla kod bloÄŸu Ã¼retme! ANCAK eÄŸer bir kod yazman veya deÄŸiÅŸtirmen isteniyorsa:\n1. **DÃœÅÃœNME AÅAMASI (THINKING STAGE):** Kod yazmadan Ã¶nce sana sunulan "Project Map", "Project Rules" ve "User Focus" (Cursor/Selection) verilerini analiz et. Stratejini 1-2 cÃ¼mleyle aÃ§Ä±kla.\n2. **KOD Ä°NCELEME MODU (REVIEW MODE):** EÄŸer kullanÄ±cÄ± bir "Ghost Review" veya refactor Ã¶nerisiyle gelmiÅŸse, koda bir kÄ±demli yazÄ±lÄ±mcÄ± (senior dev) gÃ¶zÃ¼yle bak. Sadece hatayÄ± deÄŸil, temiz kod (clean code) prensiplerini ve performansÄ± da gÃ¶zet.\n3. **HATA DÃœZELTME MODU (FIXING MODE):** EÄŸer kullanÄ±cÄ± bir terminal hatasÄ± (Terminal context) paylaÅŸmÄ±ÅŸsa, Ã¶nceliÄŸin bu hatayÄ± Ã§Ã¶zmek olsun. HatayÄ± analiz et ve doÄŸrudan Ã§Ã¶zÃ¼me odaklanan <<<SEARCH === >>>REPLACE gÃ¼ncellemeleri yap.\n4. **PROJE KURALLARI:** EÄŸer bir ".corexrules" veya "COREX.md" dosyasÄ± sunulmuÅŸsa, oradaki teknik kurallara KESÄ°NLÄ°KLE uy.\n5. **TAM FONKSÄ°YONEL KOD:** ÃœrettiÄŸin kodlar her zaman Ä°NTERAKTÄ°F olmalÄ±.\n6. **UI/UX:** Modern ve premium UI/UX prensiplerini uygula.\n7. **DOSYA GÃœNCELLEME:** Sadece deÄŸiÅŸtirmek istediÄŸin yeri <<<SEARCH === >>>REPLACE formatÄ±nda ver. Sadece zorunluysa tÃ¼m dosyayÄ± yaz.\n8. **YENÄ° DOSYA OLUÅTURMA (DÄ°KKAT!):** Kod bloÄŸunun baÅŸÄ±na MUTLAKA dosya adÄ±nÄ± yazmalÄ±sÄ±n. Ã–rnek format: ```html:index.html VEYA ```javascript:app.js. DOSYA ADI YAZMAK ZORUNLUDUR!'
    : 'You are CorexAI assistant. If the user is just chatting or saying hello, respond normally in natural language. BUT if you are generating or modifying code:\n1. **THINKING STAGE:** Before writing any code, analyze the "Project Map", "Project Rules", and "User Focus" (Cursor/Selection) provided. Explain your strategy in 1-2 sentences.\n2. **REVIEW MODE:** If a "Ghost Review" or refactor suggestion is provided, analyze the code as a senior developer. Focus on clean code principles, performance, and maintainability.\n3. **FIXING MODE:** If terminal error context is provided, prioritize fixing this specific error. Analyze the error and provide direct <<<SEARCH === >>>REPLACE updates to resolve it.\n4. **PROJECT RULES:** If a ".corexrules" or "COREX.md" file is provided, STRICTLY follow the technical rules and naming standards defined there.\n5. **FULLY FUNCTIONAL CODE:** Generated code must be INTERACTIVE.\n6. **UI/UX:** Apply modern and premium UI/UX principles.\n7. **FILE UPDATE:** Provide ONLY the exact part to change using <<<SEARCH === >>>REPLACE format. Only rewrite the full file if absolutely necessary.\n8. **NEW FILE (WARNING!):** Always provide the filename in the code block like ```html:index.html or ```javascript:app.js. FILENAME IS MANDATORY!';
};

// ğŸ†• Conversation history desteÄŸi eklendi
export async function callAI(
  message: string,
  modelId: string,
  conversationHistory?: Array<{ role: string; content: string }>,
  onStreamToken?: (text: string) => void // ğŸ†• Streaming callback
): Promise<string> {
  const isTurkish = navigator.language ? navigator.language.startsWith('tr') : true;

  // Resimleri parse et (temiz mesajÄ± al)
  const { cleanMessage, images } = parseImagesFromMessage(message);

  // History hazÄ±rla
  const messages = [...(conversationHistory || [])];

  // EÄŸer history boÅŸsa veya baÅŸÄ±nda system prompt yoksa, agentic instruction ekle
  const hasSystemPrompt = messages.some(m => m.role === 'system');
  if (!hasSystemPrompt) {
    messages.unshift({
      role: 'system',
      content: getAgenticInstruction(isTurkish)
    });
  }


  if (images.length > 0) {
    console.log('ğŸ“· Vision mode aktif:', images.length, 'resim bulundu');
  }

  // ğŸ”§ Model ID yoksa veya "default" ise, aktif bir model seÃ§
  let actualModelId = modelId;
  if (!modelId || modelId === 'default') {
    console.log('âš ï¸ Model ID belirtilmemiÅŸ, aktif model aranÄ±yor...');
    const providers = await loadAIProviders();

    // Ä°lk aktif provider'Ä±n ilk aktif modelini bul
    for (const provider of providers) {
      if (!provider.isActive) continue;

      const activeModel = provider.models.find(m => m.isActive);
      if (activeModel) {
        actualModelId = activeModel.id;
        console.log(`âœ… Aktif model bulundu: ${activeModel.displayName} (${actualModelId})`);
        break;
      }
    }

    // Hala model bulunamadÄ±ysa hata ver
    if (!actualModelId || actualModelId === 'default') {
      throw new Error('Aktif AI modeli bulunamadÄ±. LÃ¼tfen AI ayarlarÄ±ndan bir model aktif edin.');
    }
  }

  const result = await findActiveModel(actualModelId);

  if (!result) {
    throw new Error(`Model bulunamadÄ±: ${actualModelId}`);
  }

  const { provider, model } = result;

  console.log('ğŸ¤– AI Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor:', {
    modelId,
    provider: provider.name,
    model: model.displayName,
    baseUrl: provider.baseUrl,
    historyLength: conversationHistory?.length || 0
  });

  // ğŸ†• GGUF provider kontrolÃ¼ - baseUrl kontrolÃ¼ yerine provider ID kontrolÃ¼
  console.log('ğŸ” Provider kontrolÃ¼:', { id: provider.id, baseUrl: provider.baseUrl, name: provider.name });

  if (provider.id === "gguf-direct" || provider.baseUrl === "internal://gguf") {
    console.log('ğŸ“¦ GGUF provider tespit edildi, direkt GGUF Ã§aÄŸrÄ±sÄ± yapÄ±lÄ±yor...');

    // GGUF fonksiyonlarÄ±nÄ± import et
    const { getGgufModelStatus } = await import('./ggufProvider');

    // ğŸ†• GGUF model bilgisini gguf-models listesinden bul
    const ggufModels = await storage.getSettings<any[]>('gguf-models');
    let modelConfig = null;

    if (ggufModels) {
      // actualModelId ile eÅŸleÅŸen modeli bul
      modelConfig = ggufModels.find((m: any) => m.id === actualModelId);
    }

    // EÄŸer listede yoksa (yeni eklenmiÅŸ olabilir) gguf-active-model'e fallback yap (geriye dÃ¶nÃ¼k uyumluluk)
    if (!modelConfig) {
      modelConfig = await storage.getSettings<any>('gguf-active-model');
    }

    // EÄŸer config bulunamadÄ±ysa, backend'de zaten yÃ¼klÃ¼ olan modeli kullan
    if (!modelConfig || !modelConfig.localPath) {
      console.warn('âš ï¸ Config bulunamadÄ±, backend\'deki aktif model kontrol ediliyor...');
      const currentStatus = await getGgufModelStatus();
      if (currentStatus.loaded && currentStatus.loaded_models.length > 0) {
        const loadedPath = currentStatus.loaded_models[0];
        console.log('âœ… Backend\'de yÃ¼klÃ¼ model kullanÄ±lÄ±yor:', loadedPath);
        modelConfig = {
          localPath: loadedPath,
          modelName: loadedPath.split(/[\\/]/).pop()?.replace('.gguf', '') || 'gguf-model',
          contextLength: 4096
        };
      } else {
        throw new Error(`âŒ GGUF model yapÄ±landÄ±rmasÄ± veya yerel dosya yolu bulunamadÄ±: ${actualModelId}`);
      }
    }

    const config = modelConfig;
    const modelPath = config.localPath; // Backend iÃ§in asÄ±l gerekli olan yol
    console.log('ğŸ“‹ GGUF Model Path:', modelPath);

    // Model durumunu kontrol et
    const status = await getGgufModelStatus();
    console.log('ğŸ“Š GGUF Loaded Models:', status.loaded_models);

    // Model yÃ¼klÃ¼ deÄŸilse yÃ¼klemeyi dene (Otomatik yÃ¼kleme)
    if (!status.loaded_models.includes(modelPath)) {
      console.warn('âš ï¸ GGUF model henÃ¼z yÃ¼klÃ¼ deÄŸil, otomatik yÃ¼kleniyor...');
      const { loadGgufModel } = await import('./ggufProvider');
      await loadGgufModel({
        modelPath: modelPath,
        contextLength: config.contextLength || 4096,
        gpuLayers: 28, // VarsayÄ±lan GPU layer
        temperature: 0.7,
        maxTokens: 4096
      });
    }

    console.log('âœ… Model hazÄ±r, chat yapÄ±lÄ±yor...');

    // Model adÄ±ndan chat template'i belirle
    const modelName = config.modelName?.toLowerCase() || '';
    console.log('ğŸ” Model adÄ±:', modelName);

    // ğŸ”¥ Context length'i GGUF config'den al (Model Browser'dan ayarlanan deÄŸer)
    let contextLength = config.contextLength || model.maxTokens || 2048;

    // ğŸ”¥ CRITICAL FIX: Context length Ã§ok kÃ¼Ã§Ã¼kse otomatik artÄ±r
    // Kod yazarken minimum 4096 context gerekli
    if (contextLength < 4096) {
      console.warn(`âš ï¸ Context length Ã§ok kÃ¼Ã§Ã¼k (${contextLength}), 4096'ya yÃ¼kseltiliyor...`);
      contextLength = 4096;
    }

    console.log('ğŸ“ Context length (GGUF config):', contextLength);
    console.log('ğŸ” Config details:', {
      configContextLength: config.contextLength,
      modelMaxTokens: model.maxTokens,
      finalContextLength: contextLength
    });

    let fullPrompt = '';

    // Conversation history'yi hazÄ±rla (son 4 mesaj)
    const filteredHistory = conversationHistory
      ? conversationHistory.filter(msg => msg.role !== 'system').slice(-4)
      : [];

    // Model tipine gÃ¶re chat template seÃ§
    if (modelName.includes('qwen')) {
      // Qwen2.5 ChatML format: <|im_start|>role\ncontent<|im_end|>
      console.log('ğŸ“ Qwen chat template kullanÄ±lÄ±yor');

      // System prompt - Sistem diline gÃ¶re
      const systemLanguage = navigator.language || 'en';
      const isTurkish = systemLanguage.startsWith('tr');
      const systemMessage = isTurkish
        ? 'Sen Corex AI, TÃ¼rkÃ§e yanÄ±t veren bir kodlama asistanÄ±sÄ±n. KÄ±sa ve Ã¶z yanÄ±t ver. SelamlaÅŸmalarda 1-2 cÃ¼mle yeterli.'
        : 'You are Corex AI, a concise coding assistant. Keep answers SHORT and direct. For greetings, 1-2 sentences max.';

      fullPrompt += `<|im_start|>system\n${systemMessage}<|im_end|>\n`;

      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'user' : 'assistant';
        fullPrompt += `<|im_start|>${role}\n${msg.content}<|im_end|>\n`;
      }

      // Current message
      fullPrompt += `<|im_start|>user\n${cleanMessage}<|im_end|>\n<|im_start|>assistant\n`;

    } else if (modelName.includes('llama') && modelName.includes('3')) {
      // Llama 3 format
      console.log('ğŸ“ Llama 3 chat template kullanÄ±lÄ±yor');

      fullPrompt += '<|begin_of_text|>';

      // System prompt - Sistem diline gÃ¶re
      const systemLanguage = navigator.language || 'en';
      const isTurkish = systemLanguage.startsWith('tr');
      const systemMessage = isTurkish
        ? 'Sen Corex AI, TÃ¼rkÃ§e yanÄ±t veren bir kodlama asistanÄ±sÄ±n. KÄ±sa ve Ã¶z yanÄ±t ver. SelamlaÅŸmalarda 1-2 cÃ¼mle yeterli.'
        : 'You are Corex AI, a concise coding assistant. Keep answers SHORT and direct. For greetings, 1-2 sentences max.';

      fullPrompt += `<|start_header_id|>system<|end_header_id|>\n\n${systemMessage}<|eot_id|>`;

      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'user' : 'assistant';
        fullPrompt += `<|start_header_id|>${role}<|end_header_id|>\n\n${msg.content}<|eot_id|>`;
      }

      // Current message
      fullPrompt += `<|start_header_id|>user<|end_header_id|>\n\n${cleanMessage}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n`;

    } else if (modelName.includes('mistral') || modelName.includes('mixtral')) {
      // Mistral format: [INST] ... [/INST]
      console.log('ğŸ“ Mistral chat template kullanÄ±lÄ±yor');

      // Mistral doesn't use system prompt in the same way
      let conversationText = '';

      // History
      for (const msg of filteredHistory) {
        if (msg.role === 'user') {
          conversationText += `[INST] ${msg.content} [/INST] `;
        } else {
          conversationText += `${msg.content} `;
        }
      }

      // Current message
      conversationText += `[INST] ${cleanMessage} [/INST]`;

      fullPrompt = conversationText;

    } else if (modelName.includes('gemma')) {
      // Gemma format
      console.log('ğŸ“ Gemma chat template kullanÄ±lÄ±yor');

      fullPrompt += '<start_of_turn>user\n';

      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'user' : 'model';
        fullPrompt += `<start_of_turn>${role}\n${msg.content}<end_of_turn>\n`;
      }

      // Current message
      fullPrompt += `<start_of_turn>user\n${cleanMessage}<end_of_turn>\n<start_of_turn>model\n`;

    } else if (modelName.includes('phi')) {
      // Phi format
      console.log('ğŸ“ Phi chat template kullanÄ±lÄ±yor');

      // System prompt - Sistem diline gÃ¶re
      const systemLanguage = navigator.language || 'en';
      const isTurkish = systemLanguage.startsWith('tr');
      const systemMessage = isTurkish
        ? 'Sen Corex AI, TÃ¼rkÃ§e yanÄ±t veren bir kodlama asistanÄ±sÄ±n. KÄ±sa ve Ã¶z yanÄ±t ver. SelamlaÅŸmalarda 1-2 cÃ¼mle yeterli.'
        : 'You are Corex AI, a concise coding assistant. Keep answers SHORT and direct. For greetings, 1-2 sentences max.';

      fullPrompt += `<|system|>\n${systemMessage}<|end|>\n`;

      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'user' : 'assistant';
        fullPrompt += `<|${role}|>\n${msg.content}<|end|>\n`;
      }

      // Current message
      fullPrompt += `<|user|>\n${cleanMessage}<|end|>\n<|assistant|>\n`;

    } else {
      // Generic/Unknown model - simple format
      console.log('ğŸ“ Generic chat template kullanÄ±lÄ±yor (bilinmeyen model)');

      // System prompt - Sistem diline gÃ¶re
      const systemLanguage = navigator.language || 'en';
      const isTurkish = systemLanguage.startsWith('tr');
      const systemMessage = isTurkish
        ? 'Sen Corex AI, TÃ¼rkÃ§e yanÄ±t veren bir kodlama asistanÄ±sÄ±n. KÄ±sa ve Ã¶z yanÄ±t ver. SelamlaÅŸmalarda 1-2 cÃ¼mle yeterli.'
        : 'You are Corex AI, a concise coding assistant. Keep answers SHORT and direct. For greetings, 1-2 sentences max.';

      fullPrompt += `${systemMessage}\n\n`;

      // History
      for (const msg of filteredHistory) {
        const role = msg.role === 'user' ? 'User' : 'Assistant';
        fullPrompt += `${role}: ${msg.content}\n\n`;
      }

      // Current message
      fullPrompt += `User: ${cleanMessage}\n\nAssistant:`;
    }

    console.log('ğŸ”µ GGUF chat baÅŸlatÄ±lÄ±yor, prompt uzunluÄŸu:', fullPrompt.length);
    console.log('ğŸ“ Prompt preview:', fullPrompt.substring(0, 300));

    // ğŸ†• GGUF calls with timeout (FIX-25)
    const ggufTimeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => reject(new Error('GGUF yanÄ±t vermiyor (300 saniye)')), 300000);
    });

    // Chat yap - maxTokens generation iÃ§in (Ã¼retilecek token sayÄ±sÄ±)
    // Context length zaten model yÃ¼klenirken ayarlandÄ±
    // ğŸ”¥ FIXED: Minimum 2048 token garanti et, kod yazarken yeterli olsun
    const generationMaxTokens = Math.max(Math.min(contextLength / 2, 8192), 2048); // Min 2048, max 8192
    console.log('ğŸ¯ Generation max tokens:', generationMaxTokens, '(context:', contextLength, ')');

    // ğŸ†• Streaming desteÄŸi
    if (onStreamToken) {
      const { chatWithChunkedStreaming } = await import('./streamingProvider');
      const streamPromise = chatWithChunkedStreaming(
        modelPath,
        fullPrompt,
        generationMaxTokens,
        model.temperature || 0.7,
        {
          onToken: (delta) => {
            onStreamToken(delta);
          },
          onComplete: (text: string) => console.log('âœ… Streaming tamamlandÄ±:', text.length, 'karakter')
        }
      );
      const response = await Promise.race([streamPromise, ggufTimeoutPromise]);
      return sanitizeGgufResponse(response);
    }

    // Normal (non-streaming) mode
    const chatPromise = (async () => {
      const { chatWithGgufModel } = await import('./ggufProvider');
      return await chatWithGgufModel(
        modelPath,
        fullPrompt,
        generationMaxTokens,
        model.temperature || 0.7
      );
    })();

    const response = await Promise.race([chatPromise, ggufTimeoutPromise]);
    const sanitized = sanitizeGgufResponse(response);
    console.log('âœ… GGUF response alÄ±ndÄ± ve sanitize edildi, uzunluk:', sanitized.length);
    return sanitized;
  }

  // Normal provider (LM Studio, Ollama, vb.)
  // Timeout ile AI Ã§aÄŸrÄ±sÄ± (60 saniye - daha uzun cevaplar iÃ§in)
  const timeoutPromise = new Promise<never>((_, reject) => {
    setTimeout(() => reject(new Error('AI isteÄŸi zaman aÅŸÄ±mÄ±na uÄŸradÄ± (60 saniye)')), 60000);
  });

  // Temperature'Ä± biraz artÄ±r (daha yaratÄ±cÄ± ve eksiksiz cevaplar iÃ§in)
  const adjustedTemperature = model.temperature ? Math.min(model.temperature + 0.1, 0.9) : 0.7;

  // Max tokens'Ä± artÄ±r (daha uzun cevaplar iÃ§in)
  const adjustedMaxTokens = model.maxTokens ? Math.max(model.maxTokens, 8192) : 8192;

  const aiPromise = invoke<string>("chat_with_dynamic_ai", {
    message: cleanMessage,
    conversationHistory: messages, // ğŸ”¥ GÃ¼ncellenmiÅŸ history kullan
    providerConfig: {
      base_url: provider.baseUrl,
      host: provider.host || null,
      port: provider.port || null,
      api_key: provider.apiKey || null,
      model_name: model.name,
      temperature: adjustedTemperature,
      max_tokens: adjustedMaxTokens
    }
  });

  return await Promise.race([aiPromise, timeoutPromise]);
}

// Provider baÄŸlantÄ±sÄ±nÄ± test et
export async function testProviderConnection(provider: AIProvider): Promise<boolean> {
  try {
    // ğŸ†• GGUF provider iÃ§in Ã¶zel test
    if (provider.id === "gguf-direct" || provider.baseUrl === "internal://gguf") {
      console.log('ğŸ§ª GGUF provider test ediliyor...');

      // GGUF model status kontrolÃ¼
      const { getGgufModelStatus } = await import('./ggufProvider');
      const status = await getGgufModelStatus();

      console.log('ğŸ“Š GGUF Status:', status);

      // Model yÃ¼klÃ¼yse baÅŸarÄ±lÄ±
      if (status.loaded) {
        console.log('âœ… GGUF Test Sonucu: Model yÃ¼klÃ¼ - BAÅARILI');
        return true;
      }

      // Model yÃ¼klÃ¼ deÄŸilse ama config varsa uyarÄ± ver
      const hasConfig = (await storage.getSettings('gguf-active-model')) !== null;
      if (hasConfig) {
        console.log('âš ï¸ GGUF Test Sonucu: Config var ama model yÃ¼klÃ¼ deÄŸil');
        return false;
      }

      console.log('âŒ GGUF Test Sonucu: Model yapÄ±landÄ±rÄ±lmamÄ±ÅŸ');
      return false;
    }

    // Normal provider test (HTTP)
    const headers: Record<string, string> = {
      'Content-Type': 'application/json'
    };

    if (provider.apiKey) {
      headers['Authorization'] = `Bearer ${provider.apiKey}`;
    }

    // Timeout ile fetch (5 saniye)
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 5000);

    const response = await fetch(`${provider.baseUrl}/models`, {
      method: 'GET',
      headers,
      signal: controller.signal
    });

    clearTimeout(timeoutId);

    return response.ok;
  } catch (error) {
    console.error('Provider baÄŸlantÄ± testi hatasÄ±:', error);
    return false;
  }
}

// Mevcut modelleri listele (API'den)
export async function fetchAvailableModels(provider: AIProvider): Promise<string[]> {
  try {
    // ğŸ†• GGUF provider iÃ§in Ã¶zel liste
    if (provider.baseUrl === "internal://gguf") {
      console.log('ğŸ“¦ GGUF provider iÃ§in model listesi alÄ±nÄ±yor...');

      // storage'dan aktif GGUF modelini al
      const config = await storage.getSettings<any>('gguf-active-model');
      if (config) {
        console.log('âœ… GGUF Model bulundu:', config.modelName);
        return [config.modelName || 'GGUF Model'];
      }

      console.log('âš ï¸ GGUF model yapÄ±landÄ±rÄ±lmamÄ±ÅŸ');
      return [];
    }

    // Normal provider (HTTP)
    const headers: Record<string, string> = {
      'Content-Type': 'application/json'
    };

    if (provider.apiKey) {
      headers['Authorization'] = `Bearer ${provider.apiKey}`;
    }

    console.log('ğŸ” Model listesi alÄ±nÄ±yor:', provider.baseUrl);

    // Timeout ile fetch (10 saniye)
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 10000);

    const response = await fetch(`${provider.baseUrl}/models`, {
      method: 'GET',
      headers,
      signal: controller.signal
    });

    clearTimeout(timeoutId);

    console.log('ğŸ“¡ Response status:', response.status);

    if (!response.ok) {
      const errorText = await response.text();
      console.error('âŒ API hatasÄ±:', response.status, errorText);
      throw new Error(`HTTP ${response.status}: ${errorText}`);
    }

    const data = await response.json();
    console.log('ğŸ“¥ API Response:', data);

    // OpenAI format
    if (data.data && Array.isArray(data.data)) {
      const models = data.data.map((model: any) => model.id || model.name).filter(Boolean);
      console.log('âœ… Bulunan modeller:', models);
      return models;
    }

    // LM Studio format (bazen direkt array dÃ¶ner)
    if (Array.isArray(data)) {
      const models = data.map((model: any) => model.id || model.name || model).filter(Boolean);
      console.log('âœ… Bulunan modeller (array):', models);
      return models;
    }

    // Ollama format
    if (data.models && Array.isArray(data.models)) {
      const models = data.models.map((model: any) => model.name || model.id).filter(Boolean);
      console.log('âœ… Bulunan modeller (ollama):', models);
      return models;
    }

    console.warn('âš ï¸ Beklenmeyen API response formatÄ±:', data);
    return [];
  } catch (error) {
    console.error('âŒ Model listesi alÄ±namadÄ±:', error);
    throw error; // HatayÄ± yukarÄ± fÄ±rlat ki kullanÄ±cÄ± gÃ¶rebilsin
  }
}